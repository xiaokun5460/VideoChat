"""
é«˜æ€§èƒ½è½¬å½•æœåŠ¡

é›†æˆä»»åŠ¡é˜Ÿåˆ—ã€èµ„æºç›‘æ§å’Œæ™ºèƒ½ç¼“å­˜çš„è½¬å½•æœåŠ¡
"""

import asyncio
import uuid
import time
import os
import logging
from typing import Optional, List, Dict, Any
from utils.model_manager import model_manager
from utils.simple_cache import simple_cache_manager, cache_transcription_simple
from utils.task_queue import task_queue, TaskPriority
from utils.resource_monitor import resource_monitor
from middleware.error_handler import TranscriptionError
from config import AI_CONFIG, STT_CONFIG, DOWNLOAD_CONFIG


class PerformanceSTTService:
    """é«˜æ€§èƒ½è½¬å½•æœåŠ¡"""
    
    def __init__(self):
        self._initialized = False
        self._stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "queue_submissions": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "total_processing_time": 0.0
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        if self._initialized:
            return
        
        logging.info("ğŸš€ åˆå§‹åŒ–é«˜æ€§èƒ½è½¬å½•æœåŠ¡...")
        
        # å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—
        await task_queue.start()
        
        # å¯åŠ¨èµ„æºç›‘æ§
        await resource_monitor.start_monitoring()
        
        # æ·»åŠ èµ„æºç›‘æ§å›è°ƒ
        resource_monitor.add_callback(self._on_resource_update)
        
        self._initialized = True
        logging.info("âœ… é«˜æ€§èƒ½è½¬å½•æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    async def shutdown(self):
        """å…³é—­æœåŠ¡"""
        if not self._initialized:
            return
        
        logging.info("ğŸ›‘ å…³é—­é«˜æ€§èƒ½è½¬å½•æœåŠ¡...")
        
        # åœæ­¢ä»»åŠ¡é˜Ÿåˆ—
        await task_queue.stop()
        
        # åœæ­¢èµ„æºç›‘æ§
        await resource_monitor.stop_monitoring()
        
        self._initialized = False
        logging.info("âœ… é«˜æ€§èƒ½è½¬å½•æœåŠ¡å·²å…³é—­")
    
    def _on_resource_update(self, snapshot):
        """èµ„æºç›‘æ§å›è°ƒ"""
        # æ ¹æ®èµ„æºä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´
        if snapshot.memory_percent > 85:
            logging.info("âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®é‡Šæ”¾æ¨¡å‹")
            # å¯ä»¥åœ¨è¿™é‡Œå®ç°è‡ªåŠ¨æ¨¡å‹é‡Šæ”¾é€»è¾‘
    
    async def transcribe_audio_async(
        self, 
        file_path: str, 
        priority: TaskPriority = TaskPriority.NORMAL,
        use_queue: bool = True
    ) -> str:
        """
        å¼‚æ­¥è½¬å½•éŸ³é¢‘æ–‡ä»¶
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
            use_queue: æ˜¯å¦ä½¿ç”¨ä»»åŠ¡é˜Ÿåˆ—
        
        Returns:
            ä»»åŠ¡ID
        """
        if not self._initialized:
            await self.initialize()
        
        self._stats["total_requests"] += 1
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(file_path)
        cached_result = simple_cache_manager.get(cache_key)
        if cached_result:
            self._stats["cache_hits"] += 1
            logging.info(f"ğŸ”„ ç¼“å­˜å‘½ä¸­: {file_path}")
            return cached_result.get("task_id", str(uuid.uuid4()))
        
        if use_queue:
            # æäº¤åˆ°ä»»åŠ¡é˜Ÿåˆ—
            task_id = await task_queue.submit_task(
                self._transcribe_worker,
                file_path,
                priority=priority
            )
            self._stats["queue_submissions"] += 1
            return task_id
        else:
            # ç›´æ¥æ‰§è¡Œ
            return await self._transcribe_worker(file_path)
    
    def _generate_cache_key(self, file_path: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        try:
            file_stat = os.stat(file_path)
            return f"transcription:{file_path}:{file_stat.st_size}:{file_stat.st_mtime}"
        except Exception:
            return f"transcription:{file_path}:{time.time()}"
    
    async def _transcribe_worker(self, file_path: str) -> str:
        """è½¬å½•å·¥ä½œå‡½æ•°"""
        task_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            logging.info(f"ğŸ¯ å¼€å§‹è½¬å½•: {task_id} - {os.path.basename(file_path)}")
            
            # è·å–é…ç½®
            stt_config = STT_CONFIG
            
            # æ£€æŸ¥èµ„æºçŠ¶æ€
            resource_status = resource_monitor.get_current_status()
            if resource_status and resource_status.memory_percent > 90:
                raise TranscriptionError("ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œè½¬å½•", file_path)
            
            # è·å–æ¨¡å‹
            model = await model_manager.get_model()
            
            # æ‰§è¡Œè½¬å½•
            segments_generator = model.transcribe(
                file_path,
                beam_size=stt_config.get("beam_size", 5),
                language=stt_config.get("language", "zh"),
                vad_filter=stt_config.get("vad_filter", True)
            )
            
            transcription = []
            segments, info = segments_generator
            
            for segment in segments:
                transcription.append({
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text
                })
                
                # è®©å‡ºæ§åˆ¶æƒï¼Œå…è®¸å…¶ä»–ä»»åŠ¡æ‰§è¡Œ
                await asyncio.sleep(0)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            
            # ç¼“å­˜ç»“æœ
            cache_key = self._generate_cache_key(file_path)
            result_data = {
                "task_id": task_id,
                "segments": transcription,
                "processing_time": processing_time,
                "file_path": file_path,
                "completed_at": time.time()
            }
            simple_cache_manager.set(cache_key, result_data, ttl_seconds=7200)
            
            # æ›´æ–°ç»Ÿè®¡
            self._stats["completed_tasks"] += 1
            self._stats["total_processing_time"] += processing_time
            
            logging.info(f"âœ… è½¬å½•å®Œæˆ: {task_id} (è€—æ—¶: {processing_time:.2f}s)")
            return task_id
            
        except Exception as e:
            self._stats["failed_tasks"] += 1
            error_msg = str(e)
            logging.error(f"âŒ è½¬å½•å¤±è´¥: {task_id} - {error_msg}")
            raise TranscriptionError(f"è½¬å½•å¤±è´¥: {error_msg}", file_path)
        
        finally:
            # é‡Šæ”¾æ¨¡å‹å¼•ç”¨
            await model_manager.release_model()
    
    async def get_transcription_result(self, task_id: str) -> Optional[Dict]:
        """è·å–è½¬å½•ç»“æœ"""
        # é¦–å…ˆæ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€
        task_status = task_queue.get_task_status(task_id)
        if task_status:
            if task_status["status"] == "completed":
                # ä»ç¼“å­˜ä¸­æŸ¥æ‰¾ç»“æœ
                for key in simple_cache_manager._cache._cache.keys():
                    if key.startswith("transcription:"):
                        cached_data = simple_cache_manager.get(key)
                        if cached_data and cached_data.get("task_id") == task_id:
                            return cached_data
            return {"status": task_status["status"], "task_id": task_id}
        
        return None
    
    async def cancel_transcription(self, task_id: str) -> bool:
        """å–æ¶ˆè½¬å½•ä»»åŠ¡"""
        return await task_queue.cancel_task(task_id)
    
    def get_service_stats(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        queue_stats = task_queue.get_queue_stats()
        resource_stats = resource_monitor.get_statistics(minutes=5)
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = (
            self._stats["cache_hits"] / self._stats["total_requests"] * 100
            if self._stats["total_requests"] > 0 else 0
        )
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        avg_processing_time = (
            self._stats["total_processing_time"] / self._stats["completed_tasks"]
            if self._stats["completed_tasks"] > 0 else 0
        )
        
        return {
            "service_stats": {
                "total_requests": self._stats["total_requests"],
                "cache_hits": self._stats["cache_hits"],
                "cache_hit_rate": cache_hit_rate,
                "completed_tasks": self._stats["completed_tasks"],
                "failed_tasks": self._stats["failed_tasks"],
                "avg_processing_time": avg_processing_time
            },
            "queue_stats": queue_stats,
            "resource_stats": resource_stats,
            "recommendations": resource_monitor.get_memory_recommendations()
        }
    
    async def optimize_performance(self):
        """æ€§èƒ½ä¼˜åŒ–"""
        logging.info("ğŸ”§ æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–...")
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        expired_count = simple_cache_manager.cleanup_expired()
        if expired_count > 0:
            logging.info(f"ğŸ§¹ æ¸…ç†äº† {expired_count} ä¸ªè¿‡æœŸç¼“å­˜")
        
        # æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
        resource_status = resource_monitor.get_current_status()
        if resource_status:
            if resource_status.memory_percent > 80:
                logging.info("ğŸ’¾ å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®é‡Šæ”¾æ¨¡å‹")
                await model_manager.release_model()
            
            if (resource_status.gpu_memory_used_mb and 
                resource_status.gpu_memory_total_mb and
                resource_status.gpu_memory_used_mb / resource_status.gpu_memory_total_mb > 0.8):
                logging.info("ğŸ® GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜")
        
        logging.info("âœ… æ€§èƒ½ä¼˜åŒ–å®Œæˆ")


# å…¨å±€é«˜æ€§èƒ½è½¬å½•æœåŠ¡å®ä¾‹
performance_stt_service = PerformanceSTTService()


# ä¾¿æ·å‡½æ•°
async def transcribe_audio_performance(
    file_path: str, 
    priority: TaskPriority = TaskPriority.NORMAL
) -> str:
    """é«˜æ€§èƒ½è½¬å½•éŸ³é¢‘æ–‡ä»¶"""
    return await performance_stt_service.transcribe_audio_async(file_path, priority)


async def get_transcription_result_performance(task_id: str) -> Optional[Dict]:
    """è·å–è½¬å½•ç»“æœ"""
    return await performance_stt_service.get_transcription_result(task_id)
