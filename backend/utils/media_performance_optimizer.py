"""
éŸ³è§†é¢‘å¤„ç†æ€§èƒ½ä¼˜åŒ–å™¨

æä¾›Whisperè½¬å½•ä¼˜åŒ–ã€å¤§æ–‡ä»¶å¤„ç†ã€å†…å­˜ç®¡ç†ç­‰åŠŸèƒ½
"""

import asyncio
import logging
import os
import psutil
import time
from typing import Dict, Any, Optional, List, Tuple
from utils.resource_monitor import resource_monitor
from utils.task_queue import task_queue, TaskPriority
from utils.simple_cache import simple_cache_manager
from config import STT_CONFIG


class WhisperOptimizer:
    """Whisperè½¬å½•æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–Whisperä¼˜åŒ–å™¨"""
        self._stats = {
            'total_transcriptions': 0,
            'gpu_transcriptions': 0,
            'cpu_transcriptions': 0,
            'total_processing_time': 0.0,
            'memory_optimizations': 0,
            'batch_optimizations': 0
        }
    
    def get_optimal_device_config(self) -> Dict[str, Any]:
        """
        è·å–æœ€ä¼˜è®¾å¤‡é…ç½®
        
        Returns:
            ä¼˜åŒ–çš„è®¾å¤‡é…ç½®
        """
        config = STT_CONFIG.copy()
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        memory_percent = psutil.virtual_memory().percent
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # æ ¹æ®ç³»ç»Ÿè´Ÿè½½è°ƒæ•´é…ç½®
        if memory_percent > 80:
            # å†…å­˜ç´§å¼ ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®
            config['beam_size'] = min(config.get('beam_size', 5), 3)
            config['compute_type_cpu'] = 'int8'
            config['compute_type_gpu'] = 'float16'
            self._stats['memory_optimizations'] += 1
            logging.info(f"ğŸ”§ å†…å­˜ä¼˜åŒ–: è°ƒæ•´beam_size={config['beam_size']}")
        
        if cpu_percent > 90:
            # CPUè´Ÿè½½é«˜ï¼Œé™ä½å¤„ç†å¤æ‚åº¦
            config['vad_filter'] = False  # å…³é—­VADè¿‡æ»¤å‡å°‘CPUè´Ÿè½½
            logging.info("ğŸ”§ CPUä¼˜åŒ–: å…³é—­VADè¿‡æ»¤")
        
        # GPUå¯ç”¨æ€§æ£€æŸ¥
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            if gpus and config.get('device') in ['auto', 'gpu']:
                gpu = gpus[0]
                if gpu.memoryUtil > 0.8:  # GPUå†…å­˜ä½¿ç”¨è¶…è¿‡80%
                    config['device'] = 'cpu'
                    logging.info("ğŸ”§ GPUå†…å­˜ä¸è¶³ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
                else:
                    config['device'] = 'gpu'
                    logging.info(f"âœ… ä½¿ç”¨GPU: {gpu.name} (å†…å­˜ä½¿ç”¨: {gpu.memoryUtil:.1%})")
        except ImportError:
            logging.info("âš ï¸ GPUtilæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPUçŠ¶æ€")
        
        return config
    
    def estimate_processing_time(self, file_path: str) -> float:
        """
        ä¼°ç®—å¤„ç†æ—¶é—´
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
        Returns:
            ä¼°ç®—çš„å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
        """
        try:
            file_size = os.path.getsize(file_path)
            
            # åŸºäºæ–‡ä»¶å¤§å°çš„ç²—ç•¥ä¼°ç®—
            # å‡è®¾1MBéŸ³é¢‘çº¦1åˆ†é’Ÿï¼Œå¤„ç†æ—¶é—´çº¦ä¸ºéŸ³é¢‘æ—¶é•¿çš„10-30%
            estimated_duration = file_size / (1024 * 1024)  # MB
            
            config = self.get_optimal_device_config()
            if config.get('device') == 'gpu':
                processing_ratio = 0.1  # GPUå¤„ç†æ›´å¿«
            else:
                processing_ratio = 0.3  # CPUå¤„ç†è¾ƒæ…¢
            
            estimated_time = estimated_duration * 60 * processing_ratio
            return max(estimated_time, 10)  # æœ€å°‘10ç§’
            
        except Exception as e:
            logging.warning(f"âš ï¸ æ— æ³•ä¼°ç®—å¤„ç†æ—¶é—´: {str(e)}")
            return 60  # é»˜è®¤1åˆ†é’Ÿ
    
    def should_use_batch_processing(self, file_paths: List[str]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æ‰¹å¤„ç†
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        Returns:
            æ˜¯å¦ä½¿ç”¨æ‰¹å¤„ç†
        """
        if len(file_paths) < 2:
            return False
        
        total_size = sum(os.path.getsize(path) for path in file_paths if os.path.exists(path))
        available_memory = psutil.virtual_memory().available
        
        # å¦‚æœæ€»æ–‡ä»¶å¤§å°å°äºå¯ç”¨å†…å­˜çš„50%ï¼Œå¯ä»¥è€ƒè™‘æ‰¹å¤„ç†
        if total_size < available_memory * 0.5:
            self._stats['batch_optimizations'] += 1
            return True
        
        return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self._stats.copy()
        if stats['total_transcriptions'] > 0:
            stats['gpu_usage_rate'] = (stats['gpu_transcriptions'] / stats['total_transcriptions']) * 100
            stats['average_processing_time'] = stats['total_processing_time'] / stats['total_transcriptions']
        else:
            stats['gpu_usage_rate'] = 0
            stats['average_processing_time'] = 0
        
        return stats


class FileProcessingOptimizer:
    """æ–‡ä»¶å¤„ç†æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, chunk_size: int = 8192, max_memory_usage: float = 0.8):
        """
        åˆå§‹åŒ–æ–‡ä»¶å¤„ç†ä¼˜åŒ–å™¨
        
        Args:
            chunk_size: æ–‡ä»¶è¯»å–å—å¤§å°
            max_memory_usage: æœ€å¤§å†…å­˜ä½¿ç”¨ç‡
        """
        self.chunk_size = chunk_size
        self.max_memory_usage = max_memory_usage
        self._stats = {
            'files_processed': 0,
            'total_bytes_processed': 0,
            'memory_warnings': 0,
            'chunk_optimizations': 0
        }
    
    async def process_large_file(
        self,
        file_path: str,
        processor_func,
        progress_callback: Optional[callable] = None
    ) -> Any:
        """
        å¤„ç†å¤§æ–‡ä»¶ï¼ˆæ”¯æŒè¿›åº¦å›è°ƒå’Œå†…å­˜ç›‘æ§ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            processor_func: å¤„ç†å‡½æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            å¤„ç†ç»“æœ
        """
        file_size = os.path.getsize(file_path)
        self._stats['files_processed'] += 1
        self._stats['total_bytes_processed'] += file_size
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > self.max_memory_usage * 100:
            self._stats['memory_warnings'] += 1
            logging.warning(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1f}%")
            
            # ç­‰å¾…å†…å­˜é‡Šæ”¾
            await asyncio.sleep(1)
        
        # å¤§æ–‡ä»¶åˆ†å—å¤„ç†
        if file_size > 100 * 1024 * 1024:  # 100MBä»¥ä¸Š
            self._stats['chunk_optimizations'] += 1
            logging.info(f"ğŸ“¦ å¤§æ–‡ä»¶åˆ†å—å¤„ç†: {file_size / 1024 / 1024:.1f}MB")
            
            # è¿™é‡Œå¯ä»¥å®ç°æ–‡ä»¶åˆ†å—é€»è¾‘
            # ç›®å‰ç›´æ¥è°ƒç”¨å¤„ç†å‡½æ•°
        
        try:
            start_time = time.time()
            
            # è°ƒç”¨å¤„ç†å‡½æ•°
            result = await processor_func(file_path)
            
            processing_time = time.time() - start_time
            logging.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}s")
            
            if progress_callback:
                await progress_callback(100, "å¤„ç†å®Œæˆ")
            
            return result
            
        except Exception as e:
            logging.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
            if progress_callback:
                await progress_callback(-1, f"å¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    def get_optimal_chunk_size(self, file_size: int) -> int:
        """
        è·å–æœ€ä¼˜å—å¤§å°
        
        Args:
            file_size: æ–‡ä»¶å¤§å°
        
        Returns:
            æœ€ä¼˜å—å¤§å°
        """
        available_memory = psutil.virtual_memory().available
        
        # æ ¹æ®å¯ç”¨å†…å­˜å’Œæ–‡ä»¶å¤§å°è°ƒæ•´å—å¤§å°
        if file_size > available_memory * 0.1:
            # å¤§æ–‡ä»¶ä½¿ç”¨è¾ƒå°çš„å—
            return min(self.chunk_size, 4096)
        else:
            # å°æ–‡ä»¶å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å—
            return min(self.chunk_size * 2, 16384)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self._stats.copy()
        if stats['files_processed'] > 0:
            stats['average_file_size'] = stats['total_bytes_processed'] / stats['files_processed']
        else:
            stats['average_file_size'] = 0
        
        return stats


class MediaPerformanceOptimizer:
    """éŸ³è§†é¢‘æ€§èƒ½ä¼˜åŒ–å™¨ä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–éŸ³è§†é¢‘æ€§èƒ½ä¼˜åŒ–å™¨"""
        self.whisper_optimizer = WhisperOptimizer()
        self.file_optimizer = FileProcessingOptimizer()
    
    async def optimize_transcription_task(
        self,
        file_path: str,
        priority: TaskPriority = TaskPriority.NORMAL
    ) -> Dict[str, Any]:
        """
        ä¼˜åŒ–è½¬å½•ä»»åŠ¡
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
        
        Returns:
            ä¼˜åŒ–é…ç½®
        """
        # è·å–æœ€ä¼˜è®¾å¤‡é…ç½®
        device_config = self.whisper_optimizer.get_optimal_device_config()
        
        # ä¼°ç®—å¤„ç†æ—¶é—´
        estimated_time = self.whisper_optimizer.estimate_processing_time(file_path)
        
        # è·å–æœ€ä¼˜å—å¤§å°
        file_size = os.path.getsize(file_path)
        chunk_size = self.file_optimizer.get_optimal_chunk_size(file_size)
        
        optimization_config = {
            'device_config': device_config,
            'estimated_time': estimated_time,
            'chunk_size': chunk_size,
            'priority': priority,
            'file_size': file_size,
            'should_cache': file_size < 50 * 1024 * 1024,  # 50MBä»¥ä¸‹ç¼“å­˜
            'memory_monitoring': file_size > 100 * 1024 * 1024  # 100MBä»¥ä¸Šç›‘æ§å†…å­˜
        }
        
        logging.info(f"ğŸ¯ è½¬å½•ä»»åŠ¡ä¼˜åŒ–é…ç½®: {optimization_config}")
        return optimization_config
    
    async def batch_optimize_transcriptions(
        self,
        file_paths: List[str]
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡ä¼˜åŒ–è½¬å½•ä»»åŠ¡
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        Returns:
            æ‰¹é‡ä¼˜åŒ–é…ç½®
        """
        should_batch = self.whisper_optimizer.should_use_batch_processing(file_paths)
        
        if should_batch:
            logging.info(f"ğŸ“¦ å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼Œæ–‡ä»¶æ•°é‡: {len(file_paths)}")
            
            # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œå°æ–‡ä»¶ä¼˜å…ˆå¤„ç†
            sorted_files = sorted(file_paths, key=lambda x: os.path.getsize(x) if os.path.exists(x) else 0)
            
            return {
                'batch_mode': True,
                'file_order': sorted_files,
                'estimated_total_time': sum(
                    self.whisper_optimizer.estimate_processing_time(path) 
                    for path in file_paths
                ) * 0.8,  # æ‰¹å¤„ç†æ•ˆç‡æå‡20%
                'memory_monitoring': True
            }
        else:
            return {
                'batch_mode': False,
                'individual_optimization': True
            }
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'whisper_optimizer': self.whisper_optimizer.get_stats(),
            'file_optimizer': self.file_optimizer.get_stats(),
            'system_resources': {
                'memory_percent': psutil.virtual_memory().percent,
                'cpu_percent': psutil.cpu_percent(),
                'disk_usage': psutil.disk_usage('/').percent if os.name != 'nt' else psutil.disk_usage('C:').percent
            }
        }


# å…¨å±€éŸ³è§†é¢‘æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
media_performance_optimizer = MediaPerformanceOptimizer()