"""
AIæœåŠ¡æ€§èƒ½ä¼˜åŒ–å™¨

æä¾›OpenAI APIè¿æ¥æ± ã€æ™ºèƒ½ç¼“å­˜ã€æµå¼å“åº”ä¼˜åŒ–ç­‰åŠŸèƒ½
"""

import asyncio
import hashlib
import json
import logging
import time
from typing import Dict, Any, Optional, List, AsyncGenerator
from openai import AsyncOpenAI
from utils.simple_cache import simple_cache_manager
from config import AI_CONFIG


class AIConnectionPool:
    """OpenAI APIè¿æ¥æ± ç®¡ç†å™¨"""
    
    def __init__(self, max_connections: int = 5, timeout: int = 30):
        """
        åˆå§‹åŒ–è¿æ¥æ± 
        
        Args:
            max_connections: æœ€å¤§è¿æ¥æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.max_connections = max_connections
        self.timeout = timeout
        self._clients: List[AsyncOpenAI] = []
        self._semaphore = asyncio.Semaphore(max_connections)
        self._stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'total_response_time': 0.0,
            'connection_reuses': 0
        }
    
    async def _get_client(self) -> AsyncOpenAI:
        """è·å–æˆ–åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
        if self._clients:
            client = self._clients.pop()
            self._stats['connection_reuses'] += 1
            return client
        
        # åˆ›å»ºæ–°å®¢æˆ·ç«¯
        return AsyncOpenAI(
            base_url=AI_CONFIG["base_url"],
            api_key=AI_CONFIG["api_key"],
            timeout=self.timeout
        )
    
    async def _return_client(self, client: AsyncOpenAI):
        """å½’è¿˜å®¢æˆ·ç«¯åˆ°æ± ä¸­"""
        if len(self._clients) < self.max_connections:
            self._clients.append(client)
    
    async def create_completion(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False,
        use_cache: bool = True,
        cache_ttl: int = 3600,
        **kwargs
    ) -> Any:
        """
        åˆ›å»ºAIå®Œæˆè¯·æ±‚ï¼ˆæ”¯æŒè¿æ¥æ± å’Œç¼“å­˜ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼å“åº”
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            cache_ttl: ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            AIå“åº”ç»“æœ
        """
        start_time = time.time()
        self._stats['total_requests'] += 1
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = None
        if use_cache and not stream:
            cache_key = self._generate_cache_key(messages, kwargs)
            cached_result = simple_cache_manager.get(cache_key)
            if cached_result:
                self._stats['cache_hits'] += 1
                logging.info(f"ğŸ”„ AIè¯·æ±‚ç¼“å­˜å‘½ä¸­: {cache_key[:16]}...")
                return cached_result
        
        async with self._semaphore:
            client = await self._get_client()
            
            try:
                # è®¾ç½®é»˜è®¤å‚æ•°
                request_params = {
                    'model': AI_CONFIG.get("model", "gpt-3.5-turbo"),
                    'messages': messages,
                    'stream': stream,
                    **kwargs
                }
                
                # å‘é€è¯·æ±‚
                response = await client.chat.completions.create(**request_params)
                
                # è®°å½•æˆåŠŸ
                elapsed_time = time.time() - start_time
                self._stats['successful_requests'] += 1
                self._stats['total_response_time'] += elapsed_time
                
                logging.info(f"âœ… AIè¯·æ±‚å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}s")
                
                # ç¼“å­˜éæµå¼å“åº”
                if use_cache and not stream and cache_key:
                    simple_cache_manager.set(cache_key, response, cache_ttl)
                
                return response
                
            except Exception as e:
                self._stats['failed_requests'] += 1
                logging.error(f"âŒ AIè¯·æ±‚å¤±è´¥: {str(e)}")
                raise
            
            finally:
                await self._return_client(client)
    
    def _generate_cache_key(self, messages: List[Dict], kwargs: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            'messages': messages,
            'params': {k: v for k, v in kwargs.items() if k not in ['stream']}
        }
        cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        return f"ai_completion:{hashlib.md5(cache_str.encode()).hexdigest()}"
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯"""
        stats = self._stats.copy()
        if stats['total_requests'] > 0:
            stats['success_rate'] = (stats['successful_requests'] / stats['total_requests']) * 100
            stats['cache_hit_rate'] = (stats['cache_hits'] / stats['total_requests']) * 100
            stats['average_response_time'] = stats['total_response_time'] / stats['successful_requests'] if stats['successful_requests'] > 0 else 0
        else:
            stats['success_rate'] = 0
            stats['cache_hit_rate'] = 0
            stats['average_response_time'] = 0
        
        stats['active_connections'] = len(self._clients)
        stats['max_connections'] = self.max_connections
        return stats


class StreamingOptimizer:
    """æµå¼å“åº”ä¼˜åŒ–å™¨"""
    
    def __init__(self, buffer_size: int = 512, flush_interval: float = 0.05):
        """
        åˆå§‹åŒ–æµå¼ä¼˜åŒ–å™¨
        
        Args:
            buffer_size: ç¼“å†²åŒºå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            flush_interval: åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
        """
        self.buffer_size = buffer_size
        self.flush_interval = flush_interval
    
    async def optimize_stream(
        self,
        stream_generator: AsyncGenerator[str, None],
        enable_buffering: bool = True
    ) -> AsyncGenerator[str, None]:
        """
        ä¼˜åŒ–æµå¼å“åº”
        
        Args:
            stream_generator: åŸå§‹æµå¼ç”Ÿæˆå™¨
            enable_buffering: æ˜¯å¦å¯ç”¨ç¼“å†²
        
        Yields:
            ä¼˜åŒ–åçš„æµå¼æ•°æ®
        """
        if not enable_buffering:
            # ç›´æ¥é€ä¼ 
            async for chunk in stream_generator:
                yield chunk
            return
        
        buffer = ""
        last_flush = time.time()
        
        try:
            async for chunk in stream_generator:
                buffer += chunk
                current_time = time.time()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
                should_flush = (
                    len(buffer) >= self.buffer_size or
                    current_time - last_flush >= self.flush_interval or
                    chunk.endswith(('\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'))
                )
                
                if should_flush and buffer:
                    yield buffer
                    buffer = ""
                    last_flush = current_time
        
        finally:
            # å‘é€å‰©ä½™ç¼“å†²åŒºå†…å®¹
            if buffer:
                yield buffer


class AIPerformanceOptimizer:
    """AIæ€§èƒ½ä¼˜åŒ–å™¨ä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–AIæ€§èƒ½ä¼˜åŒ–å™¨"""
        self.connection_pool = AIConnectionPool(
            max_connections=AI_CONFIG.get("max_connections", 5),
            timeout=AI_CONFIG.get("timeout", 30)
        )
        self.streaming_optimizer = StreamingOptimizer()
    
    async def create_optimized_completion(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False,
        use_cache: bool = True,
        enable_stream_buffering: bool = True,
        **kwargs
    ) -> Any:
        """
        åˆ›å»ºä¼˜åŒ–çš„AIå®Œæˆè¯·æ±‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼å“åº”
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            enable_stream_buffering: æ˜¯å¦å¯ç”¨æµå¼ç¼“å†²
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ä¼˜åŒ–åçš„AIå“åº”
        """
        response = await self.connection_pool.create_completion(
            messages=messages,
            stream=stream,
            use_cache=use_cache,
            **kwargs
        )
        
        if stream:
            # ä¼˜åŒ–æµå¼å“åº”
            async def optimized_stream():
                async for chunk in response:
                    if (chunk.choices and
                        len(chunk.choices) > 0 and
                        hasattr(chunk.choices[0], 'delta') and
                        chunk.choices[0].delta.content is not None):
                        yield chunk.choices[0].delta.content
            
            return self.streaming_optimizer.optimize_stream(
                optimized_stream(),
                enable_stream_buffering
            )
        
        return response
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'connection_pool': self.connection_pool.get_stats(),
            'streaming_optimizer': {
                'buffer_size': self.streaming_optimizer.buffer_size,
                'flush_interval': self.streaming_optimizer.flush_interval
            }
        }


# å…¨å±€AIæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
ai_performance_optimizer = AIPerformanceOptimizer()