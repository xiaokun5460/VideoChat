"""
æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨

æä¾›Whisperæ¨¡å‹çš„åŠ¨æ€åŠ è½½ã€é‡Šæ”¾å’Œå†…å­˜ç®¡ç†åŠŸèƒ½
"""

import asyncio
import threading
import time
import logging
from typing import Optional
from faster_whisper import WhisperModel
from config import STT_CONFIG


class ModelManager:
    """Whisperæ¨¡å‹ç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        self._model: Optional[WhisperModel] = None
        self._last_used: float = 0
        self._model_lock = asyncio.Lock()
        self._reference_count = 0
        self._cleanup_task: Optional[asyncio.Task] = None
        self._initialized = True
        
        # é…ç½®å‚æ•°
        self.idle_timeout = 300  # 5åˆ†é’Ÿç©ºé—²åé‡Šæ”¾æ¨¡å‹
        self.cleanup_interval = 60  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    async def get_model(self) -> WhisperModel:
        """è·å–æ¨¡å‹å®ä¾‹ï¼ŒæŒ‰éœ€åŠ è½½"""
        async with self._model_lock:
            self._reference_count += 1
            self._last_used = time.time()
            
            if self._model is None:
                logging.info("ğŸ”„ æ­£åœ¨åŠ è½½Whisperæ¨¡å‹...")
                self._model = self._load_model()
                logging.info("âœ… Whisperæ¨¡å‹åŠ è½½å®Œæˆ")
                
                # å¯åŠ¨æ¸…ç†ä»»åŠ¡
                if self._cleanup_task is None or self._cleanup_task.done():
                    self._cleanup_task = asyncio.create_task(self._cleanup_loop())
            
            return self._model
    
    async def release_model(self):
        """é‡Šæ”¾æ¨¡å‹å¼•ç”¨"""
        async with self._model_lock:
            self._reference_count = max(0, self._reference_count - 1)
            self._last_used = time.time()
    
    def _load_model(self) -> WhisperModel:
        """åŠ è½½Whisperæ¨¡å‹"""
        model_size = STT_CONFIG.get("whisper_model", "base")
        device = STT_CONFIG.get("device", "auto")
        
        try:
            if device == "auto":
                # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
                logging.info("ğŸ” è‡ªåŠ¨æ£€æµ‹è®¾å¤‡...")
                try:
                    import torch
                    logging.info(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
                    if torch.cuda.is_available():
                        gpu_count = torch.cuda.device_count()
                        gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                        logging.info(f"ğŸš€ æ£€æµ‹åˆ°CUDAï¼ŒGPUæ•°é‡: {gpu_count}, GPUåç§°: {gpu_name}")
                        logging.info("ğŸ¯ ä½¿ç”¨GPUæ¨¡å¼åŠ è½½Whisperæ¨¡å‹")
                        return WhisperModel(
                            model_size,
                            device="cuda",
                            compute_type="float16"
                        )
                    else:
                        logging.info("âš ï¸ CUDAä¸å¯ç”¨ï¼ŒåŸå› å¯èƒ½æ˜¯ï¼š1)æœªå®‰è£…CUDA 2)æœªå®‰è£…GPUç‰ˆPyTorch 3)GPUé©±åŠ¨é—®é¢˜")
                except ImportError as e:
                    logging.info(f"âš ï¸ PyTorchæœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {e}")
                except Exception as e:
                    logging.info(f"âš ï¸ GPUæ£€æµ‹å¤±è´¥: {e}")

                logging.info("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼åŠ è½½Whisperæ¨¡å‹")
                return WhisperModel(
                    model_size,
                    device="cpu",
                    compute_type="int8"
                )
            else:
                # ä½¿ç”¨æŒ‡å®šè®¾å¤‡
                compute_type = "float16" if device == "cuda" else "int8"
                logging.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}, è®¡ç®—ç±»å‹: {compute_type}")
                return WhisperModel(
                    model_size,
                    device=device,
                    compute_type=compute_type
                )
                
        except Exception as e:
            logging.info(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # é™çº§åˆ°CPUæ¨¡å¼
            logging.info("ğŸ”„ é™çº§åˆ°CPUæ¨¡å¼...")
            return WhisperModel(
                model_size,
                device="cpu",
                compute_type="int8"
            )
    
    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯ï¼Œå®šæœŸæ£€æŸ¥æ˜¯å¦éœ€è¦é‡Šæ”¾æ¨¡å‹"""
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)
                
                async with self._model_lock:
                    if (self._model is not None and 
                        self._reference_count == 0 and 
                        time.time() - self._last_used > self.idle_timeout):
                        
                        logging.info("ğŸ§¹ é‡Šæ”¾ç©ºé—²çš„Whisperæ¨¡å‹ä»¥èŠ‚çœå†…å­˜")
                        del self._model
                        self._model = None
                        
                        # å¦‚æœæ²¡æœ‰å¼•ç”¨ï¼Œåœæ­¢æ¸…ç†ä»»åŠ¡
                        break
                        
            except asyncio.CancelledError:
                break
            except Exception as e:
                logging.info(f"âš ï¸ æ¨¡å‹æ¸…ç†ä»»åŠ¡é”™è¯¯: {e}")
    
    async def force_cleanup(self):
        """å¼ºåˆ¶æ¸…ç†æ¨¡å‹"""
        async with self._model_lock:
            if self._model is not None:
                logging.info("ğŸ§¹ å¼ºåˆ¶é‡Šæ”¾Whisperæ¨¡å‹")
                del self._model
                self._model = None
                self._reference_count = 0
                
            if self._cleanup_task and not self._cleanup_task.done():
                self._cleanup_task.cancel()
    
    def get_status(self) -> dict:
        """è·å–æ¨¡å‹çŠ¶æ€ä¿¡æ¯"""
        return {
            "model_loaded": self._model is not None,
            "reference_count": self._reference_count,
            "last_used": self._last_used,
            "idle_time": time.time() - self._last_used if self._last_used > 0 else 0,
            "memory_usage_mb": 0  # ç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…å¤æ‚çš„å†…å­˜è®¡ç®—
        }

    def get_stats(self) -> dict:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        return self.get_status()


# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()
