#!/usr/bin/env python3
"""
GPUæ£€æµ‹æµ‹è¯•è„šæœ¬
ç”¨äºè¯Šæ–­Whisperæ¨¡å‹GPUåŠ é€Ÿé—®é¢˜
"""

import sys
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

def test_pytorch_cuda():
    """æµ‹è¯•PyTorchå’ŒCUDA"""
    print("ğŸ” æµ‹è¯•PyTorchå’ŒCUDAç¯å¢ƒ...")
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        cuda_available = torch.cuda.is_available()
        print(f"ğŸ¯ CUDAå¯ç”¨: {cuda_available}")
        
        if cuda_available:
            # GPUè¯¦ç»†ä¿¡æ¯
            gpu_count = torch.cuda.device_count()
            print(f"ğŸš€ GPUæ•°é‡: {gpu_count}")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            # æµ‹è¯•GPUæ“ä½œ
            try:
                x = torch.tensor([1.0]).cuda()
                print(f"âœ… GPUæµ‹è¯•æˆåŠŸ: {x}")
            except Exception as e:
                print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå¯èƒ½åŸå› ï¼š")
            print("   1. æœªå®‰è£…NVIDIA GPUé©±åŠ¨")
            print("   2. æœªå®‰è£…CUDA Toolkit")
            print("   3. å®‰è£…çš„æ˜¯CPUç‰ˆæœ¬çš„PyTorch")
            print("   4. GPUä¸æ”¯æŒCUDA")
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        print("   è¯·å®‰è£…PyTorch: pip install torch")
        return False
    except Exception as e:
        print(f"âŒ PyTorchæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return cuda_available

def test_faster_whisper():
    """æµ‹è¯•faster-whisper"""
    print("\nğŸ” æµ‹è¯•faster-whisper...")
    
    try:
        from faster_whisper import WhisperModel
        print("âœ… faster-whisperå·²å®‰è£…")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½...")
        
        # å…ˆæµ‹è¯•CPUæ¨¡å¼
        try:
            model_cpu = WhisperModel("tiny", device="cpu", compute_type="int8")
            print("âœ… CPUæ¨¡å¼åŠ è½½æˆåŠŸ")
            del model_cpu
        except Exception as e:
            print(f"âŒ CPUæ¨¡å¼åŠ è½½å¤±è´¥: {e}")
        
        # å¦‚æœCUDAå¯ç”¨ï¼Œæµ‹è¯•GPUæ¨¡å¼
        if test_pytorch_cuda():
            try:
                model_gpu = WhisperModel("tiny", device="cuda", compute_type="float16")
                print("âœ… GPUæ¨¡å¼åŠ è½½æˆåŠŸ")
                del model_gpu
            except Exception as e:
                print(f"âŒ GPUæ¨¡å¼åŠ è½½å¤±è´¥: {e}")
                print("   å¯èƒ½åŸå› ï¼š")
                print("   1. faster-whisperç‰ˆæœ¬ä¸æ”¯æŒGPU")
                print("   2. CUDAç‰ˆæœ¬ä¸å…¼å®¹")
                print("   3. GPUå†…å­˜ä¸è¶³")
        
    except ImportError:
        print("âŒ faster-whisperæœªå®‰è£…")
        print("   è¯·å®‰è£…: pip install faster-whisper")
        return False
    except Exception as e:
        print(f"âŒ faster-whisperæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def test_config():
    """æµ‹è¯•é…ç½®"""
    print("\nğŸ” æµ‹è¯•VideoChaté…ç½®...")
    
    try:
        from config import STT_CONFIG
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹: {STT_CONFIG.get('whisper_model', 'unknown')}")
        print(f"   è®¾å¤‡: {STT_CONFIG.get('device', 'unknown')}")
        print(f"   è¯­è¨€: {STT_CONFIG.get('language', 'unknown')}")
        print(f"   GPUè®¡ç®—ç±»å‹: {STT_CONFIG.get('compute_type_gpu', 'unknown')}")
        print(f"   CPUè®¡ç®—ç±»å‹: {STT_CONFIG.get('compute_type_cpu', 'unknown')}")
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def test_model_manager():
    """æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨...")
    
    try:
        from utils.model_manager import model_manager
        print("âœ… æ¨¡å‹ç®¡ç†å™¨å¯¼å…¥æˆåŠŸ")
        
        # è·å–çŠ¶æ€
        status = model_manager.get_status()
        print(f"   æ¨¡å‹å·²åŠ è½½: {status['model_loaded']}")
        print(f"   å¼•ç”¨è®¡æ•°: {status['reference_count']}")
        print(f"   å†…å­˜ä½¿ç”¨: {status['memory_usage_mb']}MB")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½...")
        import asyncio
        
        async def test_load():
            model = await model_manager.get_model()
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            await model_manager.release_model()
            print("âœ… æ¨¡å‹é‡Šæ”¾æˆåŠŸ")
        
        asyncio.run(test_load())
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ VideoChat GPUæ£€æµ‹è¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    results = []
    
    # æµ‹è¯•PyTorchå’ŒCUDA
    results.append(("PyTorch/CUDA", test_pytorch_cuda()))
    
    # æµ‹è¯•faster-whisper
    results.append(("faster-whisper", test_faster_whisper()))
    
    # æµ‹è¯•é…ç½®
    results.append(("é…ç½®ç³»ç»Ÿ", test_config()))
    
    # æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨
    results.append(("æ¨¡å‹ç®¡ç†å™¨", test_model_manager()))
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    
    all_passed = True
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"   {name}: {status}")
        if not passed:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GPUåŠ é€Ÿåº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("   1. ç¡®ä¿å®‰è£…äº†NVIDIA GPUé©±åŠ¨")
    print("   2. ç¡®ä¿å®‰è£…äº†CUDA Toolkit")
    print("   3. å®‰è£…GPUç‰ˆæœ¬çš„PyTorch: pip install torch --index-url https://download.pytorch.org/whl/cu118")
    print("   4. ç¡®ä¿faster-whisperæ”¯æŒGPU: pip install faster-whisper[gpu]")

if __name__ == "__main__":
    main()
